\documentclass{article}

\newcommand{\theme}{LightTheme}

\input{../Modules/preamble.tex}

\title{Example Document}

\begin{document}

\maketitle

\section{Let's have some fun!}

\begin{theorem}{Stokes' Theorem}
    For a surface $\mathcal{S}$ defined on a region $\mathcal{D}$ with a boundary $\mathcal{C}$, in a vector field $\vec{\mathbf{F}}$, Stokes' theorem states that:
    \begin{equation}
        \begin{split}
            \oint_\mathcal{C} \vec{\mathbf{F}} \cdot \d \mathbf{r} & = \iint_\mathcal{D} (\nabla \times \vec{\mathbf{F}}) \cdot \mathbf{n} \d \sigma \\
        \end{split}
    \end{equation}
\end{theorem}

\begin{remark}{Green's Theorem \& Stokes' Theorem}
    Essentially, Green's theorem is a special case of Stokes' theorem. If $\mathcal{C}$ is a curve in the $xy$-plane, oriented clockwise, and $\mathcal{D}$ is the region in the $xy$-plane bounded by $\mathcal{C}$, then $\d \sigma = \d x \d y$ and $\mathbf{n} = \mathbf{k}$ and therefore,
    \begin{equation}
        \begin{split}
            (\nabla \times \vec{\mathbf{F}}) \cdot \mathbf{n}
             & = (\nabla \times \vec{\mathbf{F}}) \cdot (\mathbf{k})                          \\
             & = \left( \frac{\partial N}{\partial x} - \frac{\partial M}{\partial y} \right)
        \end{split}
    \end{equation}
\end{remark}

\begin{boxcontainer}{}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                title={Surface $\mathcal{S}$, its Boundary Curve $\mathcal{C}$, and Vector Field $\vec{\mathbf{F}}$},
                colorbar left,
                colormap name=viridis,
                xlabel=$x$, ylabel=$y$,
                variable=\u, variable y = \v,
                domain=0:360, y domain=0:3,
                legend pos=outer north east,
            ]
            \addplot3[
                surf,
                mesh/ordering=y varies,
            ] ( {v * cos(u)}, {v * sin(u)}, {v * v} );
            \addplot3[
                orange,
                mesh/ordering=y varies,
                -stealth,
                samples=20,
                quiver={
                        u=-x,
                        v=cos(y),
                        w=0,
                        scale arrows=0.2
                    },
            ] ( {v * cos(u)}, {v * sin(u)}, {v * v} );
            \addplot3[
                red,
                ultra thick,
                label={\mathcal{C}},
                domain=0:360,
                samples=60,
                samples y = 0,
            ] ( {3 * cos(u)}, {3 * sin(u)}, {9} );
            \legend{$\mathcal{S}$, $\vec{\mathbf{F}}$, $\mathcal{C}$}
        \end{axis}
    \end{tikzpicture}
\end{boxcontainer}

\begin{info}{Interesting Fact}
    Stokes' theorem is named after Sir George Gabriel Stokes, who formulated it in the 19th century. It has applications in various fields, including fluid dynamics, electromagnetism, and differential geometry.
\end{info}
\begin{warn}{Caution}
    When applying Stokes' theorem, ensure that the surface $\mathcal{S}$ is smooth and oriented correctly with respect to its boundary curve $\mathcal{C}$. The orientation of $\mathcal{C}$ must be consistent with the right-hand rule applied to the normal vector $\mathbf{n}$ of the surface.
\end{warn}

We might as well do some linear algebra while we're at it!
\begin{theorem}{}
    Let $\lambda_1, \lambda_2, \ldots, \lambda_k$ be distinct eigenvalues. Assume \[
        \begin{cases}
            S_1 & = \{ v_1^{(1)}, v_2^{(1)}, \ldots, v_{d_1}^{(1)} \} \\
            S_2 & = \{ v_1^{(2)}, v_2^{(2)}, \ldots, v_{d_2}^{(2)} \} \\
            \vdots                                                    \\
            S_k & = \{ v_1^{(k)}, v_2^{(k)}, \ldots, v_{d_k}^{(k)} \} \\
        \end{cases}
    \]
    where $S_i$ is a linearly independent set of eigenvectors corresponding to $\lambda$. Then, \[
        S = S_1 \cup S_2 \cup \cdots \cup S_k
    \]
    is linearly independent.
\end{theorem}
\begin{proof}{}
    Since the linear combinations of the eigenvectors of an eigenspace are still in the eigenspace, we have that \[
        \begin{cases}
            v_1 = a_1^{(1)} v_1^{(1)} + a_2^{(1)} v_2^{(1)} + \cdots + a_{d_1}^{(1)} v_{d_1}^{(1)} & \in \Nul(A - \lambda_1 I) \\
            v_2 = a_1^{(2)} v_1^{(2)} + a_2^{(2)} v_2^{(2)} + \cdots + a_{d_2}^{(2)} v_{d_2}^{(2)} & \in \Nul(A - \lambda_2 I) \\
            \vdots                                                                                 & \vdots                    \\
            v_k = a_1^{(k)} v_1^{(k)} + a_2^{(k)} v_2^{(k)} + \cdots + a_{d_k}^{(k)} v_{d_k}^{(k)} & \in \Nul(A - \lambda_k I) \\
        \end{cases}
    \]
    Then, we want to show that the only solution to \[
        v_1 + v_2 + \cdots + v_k = 0
    \]
    is the trivial solution $v_1 = v_2 = \cdots = v_k = 0$.  Applying $A$ to both sides, we have that \[
        A(v_1 + v_2 + \cdots + v_k) = \lambda_1 v_1 + \lambda_2 v_2 + \cdots \lambda_k v_k = A0 = 0.
    \]
    So, we want to solve the system (with $\lambda_i \neq \lambda_j$), \[
        \begin{cases}
            v_1 + v_2 + \cdots + v_k                               & = 0 \\
            \lambda_1 v_1 + \lambda_2 v_2 + \cdots + \lambda_k v_k & = 0 \\
        \end{cases}
    \]
    Inductively, we can show that this implies $v_1 = v_2 = \cdots = v_k = 0$. Thus, the set $S$ is linearly independent.
\end{proof}

Now, let's do some probability!
\begin{prop}{Properties of Independent Random Variables}
    \begin{enumerate}[label=(\alph*)]
        \item Let $X_1, X_2$ be independent random variables. Let $u_1, u_2 : \R \to \R$ be functions. Then, \[
                  E[u_1(X_1) \cdot u_2(X_2)] = E[u_1(X_1)] \cdot E[u_2(X_2)].
              \]
        \item Let $X_1, X_2, \ldots, X_n$ be independent random variables. Then, the linear combination of these random variables, \[
                  Y = \sum_{i = 1}^{n} a_i X_i,
              \]
              satisfies \[
                  E[Y] = \sum_{i = 1}^{n} a_i E[X_i], \quad \Var(Y) = \sum_{i = 1}^{n} a_i^2 \Var(X_i).
              \]
    \end{enumerate}
\end{prop}

\begin{prop}{}
    If $X \sim \Gamma(s, \lambda)$ and $Y \sim \Gamma(t, \lambda)$ are independent, then $X + Y \sim \Gamma(s + t, \lambda)$.
\end{prop}
\begin{proof}{}
    We know that \[
        f_X(x) = \frac{\lambda e^{-\lambda x} (\lambda x)^{s - 1}}{\Gamma(s)}, \quad 0 < x < \infty,
    \]
    and \[
        f_Y(y) = \frac{\lambda e^{-\lambda y} (\lambda y)^{t - 1}}{\Gamma(t)}, \quad 0 < y < \infty.
    \]
    Now, we will find the PDF of $X + Y$ for $0 < x < \infty$ and $0 < y < \infty$:
    \begin{equation}
        \begin{split}
            f_{X + Y}(a)
             & = \int_{0}^{a} f_X(x) f_Y(a - x) \d x                                                                                                                           \\
             & = \int_{0}^{a} \frac{\lambda e^{-\lambda x} (\lambda x)^{s - 1}}{\Gamma(s)} \cdot \frac{\lambda e^{-\lambda (a - x)} (\lambda (a - x))^{t - 1}}{\Gamma(t)} \d x \\
             & = \frac{\lambda^{s + t} e^{-\lambda a}}{\Gamma(s) \Gamma(t)} \int_{0}^{a} x^{s - a} (a - x)^{t - 1} \d x                                                        \\
        \end{split}
    \end{equation}
    Now, if we do a $u$-substitution with $u = \frac{x}{a}$, then $\d x = a \d u$. Thus,
    \begin{equation}
        \begin{split}
            f_{X + Y}(a)
             & = \frac{\lambda^{s + t} e^{-\lambda a}}{\Gamma(s) \Gamma(t)} \int_{0}^{1} (au)^{s - 1} (a - au)^{t - 1} a \d u         \\
             & = \frac{\lambda^{s + t} e^{-\lambda a}}{\Gamma(s) \Gamma(t)} a^{s + t - 1} \int_{0}^{1} u^{s - 1} (1 - u)^{t - 1} \d u \\
             & = C e^{-\lambda a} a^{s + t - 1}
        \end{split}
    \end{equation}
    Now, using the fact that \[
        \int_{-\infty}^{\infty} f_{X + Y}(a) \d a = 1,
    \]
    we can solve for $C$:
    \begin{equation}
        \begin{split}
            1 & = C \int_{0}^{\infty} e^{-\lambda a} a^{s + t - 1} \d a \\
        \end{split}
    \end{equation}
    Let $u = \lambda a$. Then, $\d u = \frac{1}{\lambda} \d a$. Thus,
    \begin{equation}
        \begin{split}
            1 & = C \int_{0}^{\infty} e^{-u} \left( \frac{u}{\lambda} \right)^{s + t - 1} \frac{1}{\lambda} \d u \\
              & = C \frac{1}{\lambda^{s + t}} \int_{0}^{\infty} e^{-u} u^{s + t - 1} \d u                        \\
              & = C \frac{\Gamma(s + t)}{\lambda^{s + t}}                                                        \\
        \end{split}
    \end{equation}
    Then, \[
        C = \frac{\lambda^{s + t}}{\Gamma(s + t)}.
    \]
    So, the PDF of $X + Y$ is given by \[
        f_{X + Y}(a) = \frac{\lambda^{s + t} e^{-\lambda a} a^{s + t - 1}}{\Gamma(s + t)}, \quad 0 < a < \infty.
    \]
    This is exactly the PDF of $\Gamma(s + t, \lambda)$.
\end{proof}

\begin{corollary}{}
    If $X_1, X_2, \ldots, X_n \sim \Exp(\lambda)$ are independent, then $X_1 + X_2 + \cdots + X_n \sim \Gamma(n, \lambda)$.
\end{corollary}

Let's finish off with a Numerical Analysis problem.
\newprob{
    Suppose we are given $n$ data points $(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$, and we want to fit them with the model \[
        f(x) = a + be^{-x}.
    \]
    The goal is to determine $a$ and $b$ by minimizing the squared error \[
        E = \sum_{i = 1}^n (a + be^{-x_i} - y_i)^2.
    \]
    Rewrite this least squares fitting problem as a system of equations in the unknowns $a$ and $b$.
}{
    First of all, we need to write the normal equations by setting the gradients to zero:
    \[
        \frac{\partial E}{\partial a} = \frac{\partial}{\partial a}\left( \sum_{i = 1}^n (a + be^{-x_i} - y_i)^2 \right) = 2 \sum_{i=1}^n (a + be^{-x_i} - y_i) = 0,
    \]
    and likewise for $b$:
    \[
        \frac{\partial E}{\partial b} = \frac{\partial}{\partial b}\left( \sum_{i = 1}^n (a + be^{-x_i} - y_i)^2 \right) = 2 \sum_{i=1}^n (a + be^{-x_i} - y_i)e^{-x_i} = 0.
    \]
    From the first equation, we have \[
        \sum_{i = 1}^n a + be^{-x_i} = \sum_{i = 1}^n y_i \implies na + b\sum_{i = 1}^n e^{-x_i} = \sum_{i = 1}^n y_i.
    \]
    And similarly, from the second equation, we have \[
        \sum_{i=1}^n a e^{-x_i} + b\sum_{i=1}^n e^{-2x_i} = \sum_{i=1}^n y_i e^{-x_i}.
    \]
    Then, in the matrix form, we want to find $a$ and $b$ such that \[
        \begin{pmatrix}
            n                     & \sum_{i=1}^n e^{-x_i}  \\
            \sum_{i=1}^n e^{-x_i} & \sum_{i=1}^n e^{-2x_i} \\
        \end{pmatrix}
        \begin{pmatrix}
            a \\
            b \\
        \end{pmatrix}
        =
        \begin{pmatrix}
            \sum_{i=1}^n y_i          \\
            \sum_{i=1}^n y_i e^{-x_i} \\
        \end{pmatrix}.
    \]
    The equivalent system of equations is therefore \[
        \begin{cases}
            na + b\sum_{i = 1}^n e^{-x_i} = \sum_{i = 1}^n y_i, \\
            a\sum_{i=1}^n e^{-x_i} + b\sum_{i=1}^n e^{-2x_i} = \sum_{i=1}^n y_i e^{-x_i}.
        \end{cases}
    \]
}

\end{document}
