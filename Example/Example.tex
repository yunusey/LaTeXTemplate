\documentclass{article}

\input{../Modules/preamble.tex}

\title{\LaTeX Template: Example Notes \\ \textit{My Notes for Linear Algebra Midterm}}

\setbool{shouldNewPage}{true}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

\begin{document}

\maketitle

\begin{boxcontainer}{Disclaimer}
	This review \emph{probably} does not cover everything discussed in Chapters 1 through 3 in detail. This is just a quick discussion of some important details about the essentials of linear algebra.
\end{boxcontainer}

\section{Linear Equations in Linear Algebra}

\subsection{System of Linear Equations}
A linear equation is defined as an equation with $n  \geq 1$ variables in the form:

\begin{equation}
	a_1x_1 + a_2x_2 + \cdots + a_nx_n = b
\end{equation}

A \emph{system of linear equations} is a collection of one or more linear equations with the same variables
$$
	x_1, x_2, \ldots, x_n
$$
involved:
\begin{equation}
	\begin{split}
		a_1x_1 + a_2x_2 + \cdots + a_nx_n & = c_1 \\
		b_1x_1 + b_2x_2 + \cdots + b_nx_n & = c_2 \\
		\vdots                            &       \\
		m_1x_1 + m_2x_2 + \cdots + m_nx_n & = c_m
	\end{split}
\end{equation}


\begin{boxcontainer}{Solution Set Types}
	A system has a \emph{solution set} which refers to the set of points where the system of linear equations are \emph{all consistent}. Two systems are \emph{equivalent} if they have the \emph{same solution set}. A system of linear equations can have three different types of solutions sets:
	\begin{enumerate}[label=\arabic*.]
		\item No solution: \textbf{Inconsistent}
		\item Unique/one solution: \textbf{Consistent}
		\item Infinitely many solutions: \textbf{Consistent}
	\end{enumerate}
	It is a little important as this shows us how a system of linear equations \emph{cannot have, say, 2 solutions}.
\end{boxcontainer}


\subsubsection{The Entrance of Matrices}
We can use matrices to \emph{represent} a system of linear equations. Let's take our previous system of linear equations:

\begin{equation}
	\begin{split}
		a_1x_1 + a_2x_2 + \cdots + a_nx_n & = c_1 \\
		b_1x_1 + b_2x_2 + \cdots + b_nx_n & = c_2 \\
		\vdots                            &       \\
		m_1x_1 + m_2x_2 + \cdots + m_nx_n & = c_m
	\end{split}
\end{equation}

We can write this, as an \emph{augmented matrix} like this:

\begin{equation}
	\begin{bmatrix}
		a_1    & a_2    & \cdots & a_n    & c_1    \\
		b_1    & b_2    & \cdots & b_n    & c_2    \\
		\vdots & \vdots & \ddots & \vdots & \vdots \\
		m_1    & m_2    & \cdots & m_n    & c_m    \\
	\end{bmatrix}
\end{equation}

Just like how we are solving the system of linear equations, we can solve the matrix---in an, arguably, easier manner. The matrix is equivalent to the original matrix as long as one does the following types of operations:

\begin{boxcontainer}{Elementary Row Operations}
	\begin{itemize}
		\item \textbf{Replacement}: Replace a row by the sum of itself and another row.
		\item \textbf{Interchange}: Interchange two rows.
		\item \textbf{Scaling}: Multiply a row by a constant.
	\end{itemize}
\end{boxcontainer}

Now that we understand a little bit more about linear algebra, let's ask the fundamental questions of linear algebra:

\begin{boxcontainer}{Fundamental Questions of Linear Algebra}
	\begin{itemize}
		\item \textbf{Existence}: Does the system of linear equations have at least one solution?
		\item \textbf{Uniqueness}: Does the system of linear equations have exactly one solution?
	\end{itemize}
\end{boxcontainer}

\subsection{Row Reduction \& Echelon Forms}

\begin{definition}{Row Echelon Form}
	We have these following requirements for a matrix to be in (row) echelon form:
	\begin{enumerate}
		\item All nonzero rows are above any rows of all zeros.
		\item Each leading entry of a row is in a column to the right of the leading entry of the row above it.
		\item All entries in a column below a leading entry are zeros.
	\end{enumerate}

	Visualization of an echelon matrix would be:

	\begin{equation}
		\begin{bmatrix}
			0 & \square & \star   & \star & \star   & \star & \star   \\
			0 & 0       & \square & \star & \star   & \star & \star   \\
			0 & 0       & 0       & 0     & \square & \star & \star   \\
			0 & 0       & 0       & 0     & 0       & 0     & \square \\
		\end{bmatrix}
		\text{ where }
		\begin{cases}
			\star   & : \text{any number, including 0} \\
			\square & : \text{any nonzero number}
		\end{cases}
	\end{equation}
\end{definition}

\begin{definition}{Reduced Row Echelon Form}
	We have these following requirements for a matrix to be in reduced row echelon form:
	\begin{enumerate}
		\item The matrix is in row echelon form.
		\item The leading entry in each nonzero row is $1$.
		\item Each leading $1$ is the only entry in its column.
	\end{enumerate}

	Visualization of an echelon matrix would be:

	\begin{equation}
		\begin{bmatrix}
			0 & 1 & 0 & \star & 0 & \star & 0 \\
			0 & 0 & 1 & \star & 0 & \star & 0 \\
			0 & 0 & 0 & 0     & 1 & \star & 0 \\
			0 & 0 & 0 & 0     & 0 & 0     & 1 \\
		\end{bmatrix}
		\text{ where }
		\begin{cases}
			\star & : \text{any number, including 0} \\
		\end{cases}
	\end{equation}

	We are also going to need to define what \emph{pivot position} and \emph{pivot column} is:

\end{definition}
\begin{definition}{Pivot Position \& Pivot Column}
	To help us understand the significance of the reduced row echelon matrix, we are also going to define the following two terms:
	\begin{enumerate}[label=\alph*)]
		\item \textbf{Pivot Position}: A position of the leading $1$ in the reduced echelon form of $A$.
		\item \textbf{Pivot Column}: A column of the leading $1$ in the reduced echelon form of $A$.
	\end{enumerate}
\end{definition}

These lead us to the following theorem:

\begin{theorem}{\textbf{Uniqueness of Reduced Row Echelon Form}}
	Each matrix is equivalent to \textbf{one and only one reduced echelon matrix}.
\end{theorem}

\begin{theorem}{Existence \& Uniqueness Theorem of Matrices}
	A linear system is \textbf{consistent} if and only if it has no columns of the form
	$$
		\begin{bmatrix} 0 & 0 & \ldots & 0 & b \end{bmatrix}
	$$

	Since if a matrix is consistent, either it has a unique solution, or it has infinitely many solutions. How do we know which one it is? Here you go:
	\begin{enumerate}[label=\alph*)]
		\item \textbf{Unique}:
		      The matrix has a unique solution if it has no free variables.
		\item \textbf{Infinite}:
		      The matrix has infinitely many solutions if it has at least one free variable.
	\end{enumerate}
\end{theorem}

\subsection{Vector Equations}
A matrix with only one column is called a \textbf{column vector} or simply a \textbf{vector}. The set of all vectors with $n$ entries is called $\R^n$.

Given vectors $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k$ in $\R^n$ and given the scalars $c_1, c_2, \ldots, c_k$, the vector $\mathbf{y}$ defined by;

\begin{equation}
	\mathbf{y} = c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + \cdots + c_k\mathbf{v}_k
\end{equation}

is called a \textbf{linear combination} of $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k$ with \textbf{weights} $c_1, c_2, \ldots, c_k$.

\begin{remark}{Vector Equations \& Augmented Matrices}
	A vector equation
	$$
		x_1\mathbf{v}_1 + x_2\mathbf{v}_2 + \cdots + x_n\mathbf{v}_n = \mathbf{b}
	$$
	has the same solution set as the augmented matrix
	$$
		\begin{bmatrix}
			\mathbf{v}_1 & \mathbf{v}_2 & \cdots & \mathbf{v}_n & \mathbf{b} \\
		\end{bmatrix}
	$$
\end{remark}

\begin{definition}{Span of Vectors}
	Let $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k \in \R^n$. Then, the \textbf{set of all linear combinations of $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k$} is denoted by
	$$
		\Span\{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k\}
	$$
	and is called the \textbf{subset of $\R^n$ spanned by $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k$}. In other words, $\Span\{ \mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k \}$ is the collection of the vectors that can be written in the form
	$$
		c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + \cdots + c_n\mathbf{v}_n
	$$
	with $c_1, c_2, \ldots, c_n$ scalars.
\end{definition}

\subsection{The Matrix Equation $A\mathbf{x} = \mathbf{b}$}
\begin{definition}{Matrix Multiplication with Vectors}
	If $A$ is an $m \times n$ matrix, with columns $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n$, and if $\mathbf{x} \in \R^n$, then the \textbf{product of $A$ and $\mathbf{x}$}, denoted by $A\mathbf{x}$, is the linear combination of the columns of $A$ using the corresponding entries in $\mathbf{x}$ as weights:

	\begin{equation}
		A\mathbf{x}
		=
		\begin{bmatrix}
			\mathbf{v}_1 & \mathbf{v}_2 & \cdots & \mathbf{v}_n \\
		\end{bmatrix}
		\begin{bmatrix}
			x_1    \\
			x_2    \\
			\vdots \\
			x_n    \\
		\end{bmatrix}
		=
		x_1\mathbf{v}_1 + x_2\mathbf{v}_2 + \cdots + x_n\mathbf{v}_n
	\end{equation}
\end{definition}
Of course, $A\mathbf{x}$ is \textbf{defined} only and only if $\text{\#columns}(A) = \text{\#rows}(\mathbf{x})$.

I think you already know what this takes us to:

\begin{theorem}{Matrix Equations, Vector Equations, and Augmented Matrices}
	\emph{This builds up to our previous remark (\nameref{rem:1}).}
	A matrix equation $A\mathbf{x} = \mathbf{b}$, where $A$ is an $m \times n$ matrix, with the columns $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{n}$, and if $\mathbf{b} \in \R^m$, the matrix equation
	$$
		A\mathbf{x} = \mathbf{b}
	$$
	has the same solution set as the vector equation
	$$
		x_1\mathbf{v}_1 + x_2\mathbf{v}_2 + \cdots + x_n\mathbf{v}_n = \mathbf{b}
	$$
	which has the same solution set as the augmented matrix
	$$
		\begin{bmatrix}
			\mathbf{v}_1 & \mathbf{v}_2 & \cdots & \mathbf{v}_n & \mathbf{b} \\
		\end{bmatrix}
	$$
\end{theorem}

\begin{info}{Existence of Solutions}
	The theorem above leads us to a very important fact: the equation $A\mathbf{x} = \mathbf{b}$ has a solution if and only if \textbf{$\mathbf{b}$ is a linear combination of the columns of $A$}.
\end{info}

Building onto our remark (\nameref{rem:1}) and our theorem above (\nameref{th:3}), we get the following theorem:

\begin{theorem}{Relation between Linear Combination \& Solution Set}
	Let $A$ be an $m \times n$ matrix. Then the following statements are either all true or all false:
	\begin{enumerate}[label=\alph*),topsep=0pt]
		\item
		      For each $\mathbf{b} \in \R^m$, the equation $A\mathbf{x} = \mathbf{b}$ has a solution.
		\item
		      Each $\mathbf{b} \in \R^m$ is a linear combination of the columns of $A$.
		\item
		      The columns of $A$ span $\R^m$.
		\item
		      $A$ has a pivot position in every row.
	\end{enumerate}
\end{theorem}

\begin{theorem}{Properties of Matrix-Vector Product}
	If $A$ is an $m \times n$ matrix, $\mathbf{u}, \mathbf{v} \in \R^n$, and $c$ is a scalar, then
	\begin{enumerate}[label=\alph*),topsep=0pt]
		\item
		      $A(\mathbf{u} + \mathbf{v}) = A\mathbf{u} + A\mathbf{v}$
		\item
		      $A(c\mathbf{u}) = cA\mathbf{u}$
	\end{enumerate}
\end{theorem}

\subsection{Solution Sets of Linear Systems}

\begin{definition}{Homogeneous Linear Systems}
	A system of linear equations is said to be \textbf{homogeneous} if it can be written in the form $A\mathbf{x} = \mathbf{0}$ where $A$ is an $m \times n$ matrix and $\mathbf{0} \in \R^m$. This system always has the \textbf{trivial solution} of $\mathbf{x} = \mathbf{0}$, where $\mathbf{0} \in \R^n$. The important question, therefore, is whether the equation has a \textbf{nontrivial solution}---the solution where $\mathbf{x} \neq \mathbf{0}$. The reason being:

	\begin{info}{Nontrivial Solution}
		The homogeneous equation $A\mathbf{x} = \mathbf{0}$ has a nontrivial solution if and only if the equation has \emph{at least 1 free variable}.
	\end{info}
\end{definition}

We can use the \textbf{parametric vector equations} to denote the solution vectors $\mathbf{x}$:
$$
	\mathbf{x}= s\mathbf{u} + t\mathbf{v}
$$

\begin{definition}{Nonhomogeneous Linear Systems}
	Nonhomogeneous linear systems are linear systems in the form $A\mathbf{x} = \mathbf{b}$ where $A$ is an $m \times n$ matrix and $\mathbf{b} \neq \mathbf{0} \in \R^m$.
\end{definition}

\begin{theorem}{Solution Set to Nonhomogeneous Linear Systems}
	Suppose the equation $A\mathbf{x} = \mathbf{b}$ is consistent for some $\mathbf{b}$ given, and let $\mathbf{p}$ be a solution ($A\mathbf{p} = \mathbf{b}$). Then the solution set of $A\mathbf{x} = \mathbf{b}$ is the set of all vectors of the form $\mathbf{w} = \mathbf{p} + \mathbf{v}_h$ where $\mathbf{v}_h$ is any solution of the homogeneous equation $A\mathbf{x} = \mathbf{0}$.

	\begin{boxcontainer}{Quick Proof}
		We are given
		$$
			A\mathbf{p} = \mathbf{b} \text{ and } A\mathbf{v}_h = \mathbf{0}
		$$
		Then,
		$$
			A\left( \mathbf{p} + \mathbf{v}_h \right) = \underbrace{A\mathbf{p}}_{\mathbf{b}} + \underbrace{A\mathbf{v}_h}_{\mathbf{0}} = \mathbf{b}
		$$
		which indeed is a solution to $A\mathbf{x} = \mathbf{b}$.
	\end{boxcontainer}
\end{theorem}

\subsection{Applications of Linear Algebra (\emph{Redacted})}

\subsection{Linear Independence}

\begin{definition}{Linear Independence v. Dependence}
	An indexed set of vectors $\{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_p\}$ in $\R^n$ is said to be \textbf{linearly independent} if the vector equation
	$$
		x_1\mathbf{v}_1 + x_2\mathbf{v}_2 + \ldots + x_p\mathbf{v}_p = \mathbf{0}
	$$
	has only \textbf{trivial solution}. The set of vectors $\{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_p\}$ is said to be \textbf{linearly dependent} if there exist weights $c_1, c_2, \ldots, c_p$ such that
	$$
		c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + \ldots + c_p\mathbf{v}_p = \mathbf{0}
	$$
	where there exists \textbf{at least one $c_i \neq 0$ for all $i \in \{1, 2, \ldots, p\}$}.
\end{definition}

With all of our knowledge of \emph{vectors} and \emph{matrices}, let's observe:

\begin{remark}{Linear Independence: Vector \& Matrix Equations}
	Let $A$ be an $m \times n$ matrix with columns $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n$, and let $\mathbf{x} \in \R^n$. Then,
	\begin{enumerate}[label=\alph*),topsep=0pt]
		\item
		      RREF of $\begin{bmatrix} \mathbf{v}_1 & \mathbf{v}_2 & \cdots & \mathbf{v}_n & \mathbf{0} \end{bmatrix}$, \textbf{has no free variables} if the vectors are \textbf{linearly independent}.
		\item
		      The equation $x_1\mathbf{v}_1 + x_2\mathbf{v}_2 + \cdots + x_n\mathbf{v}_n = \mathbf{0}$ has only the \textbf{trivial solution} $\mathbf{x} = \mathbf{0}$ if the vectors are \textbf{linearly dependent}.
		\item
		      The columns of $A$ are \textbf{linearly independent} if only and only if $A\mathbf{x} = \mathbf{0}$ has only the \textbf{trivial solution}.
	\end{enumerate}
\end{remark}

These discussions will lead to three new theorems, all of them essentially pointing to the same statement.

\begin{theorem}{Characterization of Linearly Dependent Sets}
	An indexed set $S = \{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k\}$ of two or more vectors is linearly dependent if and only if at least one of the vectors, some $\mathbf{v}_i \in S$ is a linear combination of the others.

	\emph{\textbf{Warning}: This doesn't mean that \textbf{all the vectors} in $S$ are linear combinations of the preceding vectors.}
\end{theorem}

\begin{theorem}{Linear Dependence in $\R^n$}
	If a set contains more vectors than there are entries in each vector, for $\{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k\}$ in $\R^n$ where $k > n$, the vectors in the set are linearly dependent.
\end{theorem}

\begin{theorem}{Zero Vector in a Set $S$}
	If a set $S = \{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k\} \in \R^n$ contains the zero vector, then the set is \emph{linearly dependent}.

	\begin{boxcontainer}{Quick Proof}
		Let $S = \{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k\}$ with $\mathbf{0} \in S$. Since
		\begin{equation}
			\mathbf{0} = x_1\mathbf{v}_1 + x_2\mathbf{v}_2 + \cdots + x_n\mathbf{v}_n
		\end{equation}
		for $x_1 = x_2 = \cdots = x_n = 0$, the set is linearly dependent.
	\end{boxcontainer}
\end{theorem}

\subsection{Introduction to Linear Transformations}

\begin{definition}{Linear Transformations}
	A transformation $T$ from $\R^n$ to $\R^m$ is a rule that assigns to each vector $\mathbf{x} \in \R^n$ a vector $T(\mathbf{x}) \in \R^m$. We also call $\R^n$ the \textbf{domain} and $\R^m$ the \textbf{codomain} of $T$, which we use the notation $T: \R^n \to \R^m$ to denote it.

	Let $T(\mathbf{x}) = A\mathbf{x}$ be a linear transformation that maps $\R^n$ to $\R^m$, which means that the matrix $A$ is an $m \times n$ matrix. Then, we sometimes also denote this linear transformation as $\mathbf{x} \mapsto A\mathbf{x}$.
\end{definition}

\begin{boxcontainer}{Properties of Linear Transformations}
	A transformation $T$ is \textbf{linear} if

	\begin{enumerate}[label=\alph*)]
		\item
		      $T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v})$ for all $\mathbf{u}, \mathbf{v}$ in the domain of $T$
		\item
		      $T(c\mathbf{u}) = cT(\mathbf{u})$ for all scalars $c$ and $\mathbf{u}$ in the domain of $T$
	\end{enumerate}
\end{boxcontainer}

\begin{remark}{Outcomes of Properties of Linear Transformations}
	\begin{enumerate}[label=\alph*)]
		\item
		      A linear transformation $T$ \emph{always} maps $\mathbf{0}$ to $\mathbf{0}$ in the domain of $T$:
		      $$
			      T(\mathbf{0}) = \mathbf{0}
		      $$
		      I wrote this fact in one of my proofs in one of the previous assignments---it is very powerful to know.
		\item
		      For all scalars $c$ and $d$, and all vectors $\mathbf{u}, \mathbf{v}$ in the domain of $T$, we have
		      $$
			      T(c\mathbf{u} + d\mathbf{v}) = cT(\mathbf{u}) + dT(\mathbf{v})
		      $$
	\end{enumerate}
\end{remark}

\subsection{The Matrix of a Linear Transformation}
Honestly, I think this is the subsection of the chapter where you understand the power of linear algebra. Basically, let's say you have a linear transformation that is represented geometrically or described in words. As a mathematician, you want to have a formula $T(\mathbf{x})$ to describe this transformation. What linear algebra tells us to that is that if you know what a linear transformation does to the identity matrix $\begin{bmatrix} \mathbf{e}_1 & \mathbf{e}_2 & \cdots & \mathbf{e}_n \end{bmatrix}$, then you know the linear transformation matrix $A$ where $\mathbf{x} \mapsto A\mathbf{x}$ is your linear transformation $T$. The following theorem indicates this fact:

\begin{theorem}{The Matrix of a Linear Transformation}
	Let $T: \R^n \to \R^m$ be a linear transformation. Then there exists a unique matrix $A$ such that
	$$
		T(\mathbf{x}) = A\mathbf{x} \text{ for all } \mathbf{x} \in \R^n
	$$
	In fact, $A$ is the $m \times n$ matrix whose $j$th column is the vector $T(\mathbf{e}_j)$:
	$$
		A = \begin{bmatrix} T(\mathbf{e}_1) & T(\mathbf{e}_2) & \cdots & T(\mathbf{e}_n) \end{bmatrix}
	$$
	We will, from now on, call the matrix $A$ the \textbf{standard matrix for the linear transformation $T(\mathbf{x})$}.

	%% TODO: Add proof if you have time
\end{theorem}

We will define two new concepts as we are discussing linear transformations:
\begin{definition}{Onto \& One-to-One}
	\begin{enumerate}
		\item
		      \textbf{Onto (\emph{Existence})}: A mapping $T: \R^n \to \R^m$ is said to be \textbf{onto} $\R^m$ if each $\mathbf{b} \in \R^m$ is the image of \emph{at least one $\mathbf{x}$ in $\R^n$}. In other words, $T$ is onto $\R^m$ if the range of $T$ is the entirety of $\R^m$.
		      \begin{warn}{Remember!}
			      If $m > n$ where $T: \R^n \to \R^m$, then $T$ is not onto $\R^m$---it is impossible. A linear transformation can have a dimension of at most $n$.
		      \end{warn}
		\item
		      \textbf{One to One (\emph{Uniqueness})}: A mapping $T: \R^n \to \R^m$ is said to be \textbf{one-to-one} if each $\mathbf{b} \in \R^m$ is the image of \emph{at most one $\mathbf{x} \in \R^n$}.

		      \begin{warn}{Remember!}
			      $T$ is one-to-one if and only if, for each $\mathbf{b} \in \R^m$, the equation $A\mathbf{x} = \mathbf{b}$, the equation $T(\mathbf{x}) = \mathbf{b}$ has either a unique solution or none at all.
		      \end{warn}
	\end{enumerate}
\end{definition}
These discussions will lead to a new theorem:

\begin{theorem}{Uniqueness of a Linear Transformation}
	Let $T: \R^n \to \R^m$ be a linear transformation. Then, $T$ is one-to-one if and only if the equation $T(\mathbf{x}) = \mathbf{0}$ has only the trivial solution.
\end{theorem}

These will lead us to our next theorem:

\begin{theorem}{Linear Independence on Existence \& Uniqueness}
	Let $T: \R^n \to \R^m$ be a linear transformation, and let $A$ be the standard matrix for $T$. Then,

	\begin{enumerate}[label=\alph*)]
		\item
		      $T$ maps $\R^n$ onto $\R^m$ if and only if the columns of $A$ span $\R^m$.
		\item
		      $T$ is one-to-one if and only if the columns of $A$ are linearly independent.
	\end{enumerate}
\end{theorem}

There four special transformations that can be visualized in 2D (which I won't for now):

\begin{enumerate}[label=\alph*)]
	\item
	      \textbf{Reflections}
	\item
	      \textbf{Contractions \& Expansions}
	\item
	      \textbf{Shears}
	\item
	      \textbf{Projections}
\end{enumerate}

\subsection{Linear Models in Business, Science, and Engineering (\emph{Redacted})}

\section{Matrix Algebra}

\subsection{Matrix Operations}
If $A$ is an $m \times n$ matrix, then the scalar entry in the $i$th row and $j$th column of $A$ is denoted by $a_{ij}$ and is called the $(i, j)$-entry of $A$.

The \textbf{diagonal entries} in an $m \times n$ matrix $A =  \begin{bmatrix} a_{ij} \end{bmatrix}$ are $a_{11}, a_{22}, \ldots$ and form the \textbf{main diagonal} of $A$. A \textbf{diagonal matrix} is a square $n \times n$ matrix whose nondiagonal entries are $0$, for instance $I_n$.

\begin{theorem}{Properties of Scalar Sums and Products of Matrices}
	Let $A$, $B$, and $C$ be matrices of the same size, and let $r$ and $s$ be scalars.

	\begin{enumerate}[label=\alph*)]
		\item
		      $A + B = B + A$
		\item
		      $(A + B) + C = A + (B + C)$
		\item
		      $A + 0 = A$
		\item
		      $r(A + B) = rA + rB$
		\item
		      $(r + s)A = rA + sA$
		\item
		      $(r + s)A = rA + sA$
		\item
		      $r(sA) = (rs)A$
	\end{enumerate}
\end{theorem}

\begin{definition}{Matrix Multiplication}
	If $A$ is an $m \times n$ matrix, and if $B$ is an $n \times p$ matrix with columns $\mathbf{b}_1, \mathbf{b}_2, \ldots, \mathbf{b}_p$, then the product $AB$ is the $m \times p$ matrix whose columns are $A\mathbf{b}_1, A\mathbf{b}_2, \ldots, A\mathbf{b}_p$:

	$$
		AB = A\begin{bmatrix} \mathbf{b}_1 & \mathbf{b}_2 & \cdots & \mathbf{b}_p \end{bmatrix} = \begin{bmatrix} A\mathbf{b}_1 & A\mathbf{b}_2 & \cdots A\mathbf{b}_p \end{bmatrix}
	$$

	\begin{info}[colback=blue!10]{Meaning Behind Multiplication}
		Essentially, the matrix multiplication $AB$ corresponds to composition of linear transformations.
	\end{info}
\end{definition}

\begin{theorem}{Properties of Matrix Multiplication}
	Let $A$ be an $n \times m$ matrix, and let $B$ and $C$ have sizes for which the indicated sums and products are defined.
	\begin{enumerate}[label=\alph*)]
		\item \textbf{Associativity}:
		      $A(BC) = (AB)C$
		\item \textbf{Distributivity}:
		      $A(B + C) = AB + AC$
		\item \textbf{Commutativity}:
		      $(B + C)A = BA + CA$
		\item \textbf{Scalar Multiplication}:
		      $r(AB) = (rA)B = A(rB)$
		\item \textbf{Identity}:
		      $I_mA = A = AI_n$
	\end{enumerate}
\end{theorem}

We also define powers of matrices and transposes of matrices:
\begin{definition}{Powers of a Matrix}
	Let $A$ be an $n \times n$ matrix and $k$ be a positive integer. Then,
	$$
		A^k = \underbrace{A\cdots A}_{k \text{ times}}
	$$
\end{definition}

\begin{definition}{Transpose of a Matrix}
	Given an $m \times n$ matrix $A$, the \textbf{transpose} of $A$ is the $n \times m$ matrix denoted by $A^T$, whose columns are formed from the corresponding rows of $A$.

	$$
		A = \begin{bmatrix}
			a & b & c & d & e \\
			f & g & h & i & j \\
			k & l & m & n & o \\
			p & r & q & s & t \\
		\end{bmatrix}
		\implies
		A^T = \begin{bmatrix}
			a & f & k & p \\
			b & g & l & r \\
			c & h & m & q \\
			d & i & n & s \\
			e & j & o & t \\
		\end{bmatrix}
	$$
\end{definition}

\begin{theorem}{Properties of Transpose}
	Let $A$ and $B$ denote matrices whose sizes are appropriate for the indicated sums and products.
	\begin{enumerate}[label=\alph*)]
		\item
		      $(A^T)^T = A$
		\item
		      $(A + B)^T = A^T + B^T$
		\item
		      $(rA)^T = rA^T$
		\item
		      $(AB)^T = B^T A^T$
	\end{enumerate}
\end{theorem}

\subsection{The Inverse of a Matrix}
An $n \times n$ matrix is said to be \textbf{invertible} if there is an $n \times n$ matrix $B$ such that $AB = I_n$.

\begin{definition}{Inverse of a Matrix}
	Let $A$ be an $n \times n$ matrix. Then the inverse of the matrix, denoted by $A^{-1}$, is the matrix such that $A^{-1}A = I_n$ and $AA^{-1} = I_n$.

	\begin{enumerate}[label=\alph*)]
		\item
		      \textbf{Singular Matrix}: A matrix that is not invertible.
		\item
		      \textbf{Non-singular Matrix}: A matrix that is invertible.
	\end{enumerate}
\end{definition}

\begin{theorem}{Inverse of a $2 \times 2$ matrix}
	Let $A = \begin{bmatrix}
			a & b \\
			c & d \\
		\end{bmatrix}$. If $\det A = ad - bc \neq 0$, then $A$ is invertible and
	$$
		A^{-1} = \underbrace{\frac{1}{ad - bc}}_{\det A} \begin{bmatrix}
			d  & -b \\
			-a & c  \\
		\end{bmatrix}
	$$
\end{theorem}

The definition of the inverse matrix will lead to very fancy stuff, one of them being finding solutions for the matrix equation $A\mathbf{x} = \mathbf{b}$.

\begin{theorem}{Solutions of Matrix Equations Using Inverse}
	If $A$ is an invertible $n \times n$ matrix, then for each $\mathbf{b} \in \R^n$, the equation $A\mathbf{x} = \mathbf{b}$ has the \emph{unique solution} $\mathbf{x} = A^{-1}\mathbf{b}$.

	\begin{boxcontainer}{Quick Proof}
		Let $A$ be an invertible matrix and we want to solutions to the matrix equation $A\mathbf{x} = \mathbf{b}$.
		\begin{equation}
			\begin{split}
				A\mathbf{x}       & = \mathbf{b}       \\
				A^{-1}A\mathbf{x} & = A^{-1}\mathbf{b} \\
				\mathbf{x}        & = A^{-1}\mathbf{b}
			\end{split}
		\end{equation}
	\end{boxcontainer}
\end{theorem}

With all of our knowledge involved in transposes and inverses of matrices, we can write the useful properties of the matrices:

\begin{theorem}{Properties of Invertible Matrices}
	\begin{enumerate}[label=\alph*)]
		\item
		      If $A$ is an invertible matrix, then $A^{-1}$ is invertible and
		      $$
			      (A^{-1})^{-1} = A
		      $$
		\item
		      If $A$ and $B$ are $n \times n$ invertible matrices, then so is $AB$, and the inverse of $AB$ is
		      $$
			      (AB)^{-1} = B^{-1}A^{-1}
		      $$
		\item
		      If $A$ is an invertible matrix, then $A^T$ is also an invertible matrix and
		      $$
			      (A^T)^{-1} = (A^{-1})^T
		      $$
	\end{enumerate}
\end{theorem}

\begin{definition}{Elementary Matrices}
	An \textbf{elementary matrix} is obtained by performing a single elementary row operation on an identity matrix. There are three types of elementary matrices:

	\begin{enumerate}
		\item
		      \textbf{Row Swap}: An elementary matrix that swaps two rows. (ex: $\begin{bmatrix}
				      0 & 1 \\
				      1 & 0 \\
			      \end{bmatrix}$ swaps $R_1$ and $R_2$)
		\item
		      \textbf{Row Addition}: An elementary matrix that adds a multiple of one row to another row (ex: $\begin{bmatrix}
				      1 & k \\
				      0 & 1 \\
			      \end{bmatrix}$ adds $k$ times $R_2$ to $R_1$)
		\item
		      \textbf{Row Scaling}: An elementary matrix that multiplies one row by a constant (ex: $\begin{bmatrix}
				      k & 0 \\
				      0 & 1 \\
			      \end{bmatrix}$ scales $R_1$ by $k$)
	\end{enumerate}

	\emph{\textbf{Note}: An elementary matrix is always invertible.}
\end{definition}
We will use this definition of elementary matrices to find the inverse of a matrix. In order to do so, we need the next theorem:
\begin{theorem}{Invertible Matrix $A$ is Row Equivalent to $I_n$}
	An $n \times n$ matrix $A$ is \textbf{invertible only and only if $A$ is row equivalent to $I_n$}, and in this case, the sequence of elementary row operations that reduces $A$ to $I_n$ also transforms $I_n$ to $A$.

	\begin{boxcontainer}{Quick Proof}
		Let $A$ be an invertible matrix and we know that:
		$$
			\underbrace{E_{p} E_{p - 1} \cdots E_1}_{\text{Elementary Matrices}}A = I_n
		$$
		Then,
		$$
			\underbrace{E_{p} E_{p - 1} \cdots E_1}_{\text{Elementary Matrices}} \underbrace{A A^{-1}}_{I_n} = \underbrace{I_n A^{-1}}_{A^{-1}}
		$$
		simplifies to
		$$
			E_p E_{p - 2} \cdots E_1 I_n = A^{-1}
		$$
	\end{boxcontainer}
\end{theorem}

So, now that we know the sequence to row reduce the matrix $A$ to $I_n$, when applied to $I_n$, leads to $A^{-1}$, we can use this fact to find an algorithm!

\begin{boxcontainer}{Algorithm for Finding the Inverse of a Matrix}
	Row reduce the augmented matrix $\begin{bmatrix} A & I \end{bmatrix}$. If $A$ is row equivalent to $I$, then $\begin{bmatrix} A & I \end{bmatrix}$ is row equivalent to $\begin{bmatrix} I & A^{-1} \end{bmatrix}$. Otherwise, (say, there is a free variable which directly indicates that the matrix is not invertible), the matrix is not invertible.
\end{boxcontainer}

\subsection{Characterizations of Invertible Matrices}
This subsection is the point where all of our knowledge from the first section and the second section merge into a very important theorem:

\begin{theorem}{Invertible Matrix Theorem}
	Let $A$ be an invertible matrix. Then the following statements are equivalent. That is, for given $A$, the statements are either all true, or all false.
	\begin{enumerate}[label=\alph*)]
		\item
		      $A$ is invertible.
		\item
		      $A$ is row equivalent to $I_n$.
		\item
		      $A$ has $n$ pivot positions.
		\item
		      The equation $A\mathbf{x} = \mathbf{0}$ has only the trivial solution.
		\item
		      The columns of $A$ form a linearly independent set.
		\item
		      The linear transformation $\mathbf{x} \mapsto A\mathbf{x}$ is one-to-one.
		\item
		      The equation $A\mathbf{x} = \mathbf{b}$ has at least one solution for each $\mathbf{b} \in \R^n$ (\emph{Repetition of "onto"}).
		\item
		      The columns of $A$ span $\R^n$ (\emph{Repetition of "one-to-one"}).
		\item
		      The linear transformation $\mathbf{x} \mapsto A\mathbf{x}$ maps $\R^n$ onto $\R^n$.
		\item
		      There is an $n \times n$ matrix $C$ such that $CA = I$ (\emph{Repetition of "inverse"}).
		\item
		      There is an $n \times n$ matrix $D$ such that $AD = I$ (\emph{Repetition of "inverse"}).
		\item
		      $A^T$ is an invertible matrix.
	\end{enumerate}
\end{theorem}

We know from the previous section that matrix multiplications are essentially compositions of linear transformations. So, we define a linear transformation $T: \R^n \to \R^n$ invertible if there exists a function $S: \R^n \to \R^n$ such that $S(T(\mathbf{x})) = \mathbf{x}$ and $T(S(\mathbf{x})) = \mathbf{x}$ for all $\mathbf{x} \in \R^n$, which takes us to our next theorem:

\begin{theorem}{Invertible Linear Transformations}
	Let $T: \R^n \to \R^n$ be a linear transformation and let $A$ be a standard matrix for $T$. Then $T$ is invertible if and only if $A$ is an invertible matrix. In that case, the linear transformation $S$ given by $S(\mathbf{x}) = A^{-1}\mathbf{x}$ is the unique function satisfying

	\begin{enumerate}[label=\alph*)]
		\item
		      $S(T(\mathbf{x})) = \mathbf{x}$ for all $\mathbf{x} \in \R^n$.
		\item
		      $T(S(\mathbf{x})) = \mathbf{x}$ for all $\mathbf{x} \in \R^n$.
	\end{enumerate}
\end{theorem}
\emph{The rest of the section was not part of the exam, and therefore is omitted.}

\section{Determinants}

\subsection{Introduction to Determinants}
\begin{definition}{Determinant of a Matrix}
	For $n \geq 2$, the \textbf{determinant} of an $n \times n$ matrix $A = \begin{bmatrix} a_{ij}\end{bmatrix}$ is given by
	\begin{equation}
		\begin{split}
			\det A & = a_{11}\det A_{11} - a_{12}\det A_{12} + \cdots + a_{1n}\det A_{1n} \\
			       & = \sum_{j = 1}^{n} a_{1j}\det A_{1j}
		\end{split}
	\end{equation}
	where the equation is called \textbf{cofactor expansion across the first row}. We are going to talk in detail what \emph{cofactor expansion} is in the following theorem.
\end{definition}

\begin{theorem}{Cofactor Expansion Across a Row or Down a Column}
	The determinant of an $n \times n$ matrix $A$ can be computed by a cofactor expansion across any row or down any column. We will denote the \textbf{$(i, j)$-th cofactor} of $A$ by $C_{ij}$. That is,
	\begin{equation}
		C_{ij} = (-1)^{i+j}\det A_{ij}
	\end{equation}
	The expansion across the $i$-th row using the cofactors is
	\begin{equation}
		\begin{split}
			\det A = a_{i1}C_{i1} + a_{i2}C_{i2} + \cdots + a_{in}C_{in}
		\end{split}
	\end{equation}
	and the cofactor expansion down the $j$-th column is
	\begin{equation}
		\begin{split}
			\det A = a_{1j}C_{1j} + a_{2j}C_{2j} + \cdots + a_{nj}C_{nj}
		\end{split}
	\end{equation}
\end{theorem}

\begin{theorem}{Determinant of a Triangular Matrix}
	If $A$ is a triangular matrix, then the determinant is the product of the entries on the main diagonal.
\end{theorem}

\subsection{Properties of Determinants}
\begin{theorem}{Row Operations \& Their Effect on Determinants}
	Let $A$ be a square matrix ($n \times n$). Then,

	\begin{enumerate}[label=\alph*)]
		\item \textbf{Row Swap}:
		      If two rows of $A$ are interchanged/swapped to produce a matrix $B$, then $\det B = -\det A$.
		\item \textbf{Row Addition}:
		      If a multiple of one row of $A$ is added to another row to produce a matrix $B$, then $\det B = \det A$.
		\item \textbf{Row Multiplication}:
		      If one row of $A$ is multiplied by $k$ to produce $B$, then $\det B = k \det A$.
	\end{enumerate}
\end{theorem}

\begin{theorem}{Determinant \& Inversible Matrices}
	Let $A$ be an $n \times n$ matrix. Then $A$ is invertible if and only if $\det A \neq 0$.
\end{theorem}

Unlike the previous sections, where we already pretty much focused on the effects of row operations on matrices, now we will also take a look at the column operations:

\begin{theorem}{Column Operations \& Their Effect on Determinants}
	If $A$ is an $n \times n$ matrix, then $\det A^T = \det A$, and therefore, the column operations, which essentially corresponds to operations done on the transpose of $A$ and transpose it back to $A$ have the same effects as the row operations done on $A$.
\end{theorem}
Here comes a very important property of determinants:
\begin{theorem}{Multiplicative Property}
	If $A$ and $B$ are $n \times n$ matrices, then $\det AB = \det A \det B$.
\end{theorem}

\end{document}
